{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c7aa2c2-0a59-4370-be62-757d9e2d0bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import onnx\n",
    "from onnx import numpy_helper\n",
    "\n",
    "from StyleTransferModel_128 import StyleTransferModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ec096b36-d943-4822-8e3a-819524fd6128",
   "metadata": {},
   "outputs": [],
   "source": [
    "INSWAPPER = False # set to false if converting from old reswapper architecture weights, set to true if converting from inswapper_128.onnx\n",
    "WEIGHTS_PATH = \"models/reswapper-1019500.pth\" # file ending .pt, .pth, .onnx\n",
    "# WEIGHTS_PATH = \"models/inswapper_128_batched.onnx\" # file ending .pt, .pth, .onnx\n",
    "\n",
    "transfer_weights = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "cea391d3-14ed-40f2-983f-dc2302386218",
   "metadata": {},
   "outputs": [],
   "source": [
    "if WEIGHTS_PATH.endswith('.onnx'):\n",
    "    onnx_model   = onnx.load(WEIGHTS_PATH)\n",
    "    INTIALIZERS  = onnx_model.graph.initializer\n",
    "    transfer_weights = {}\n",
    "    for initializer in INTIALIZERS:\n",
    "        W = numpy_helper.to_array(initializer)\n",
    "        transfer_weights[initializer.name] = W\n",
    "elif WEIGHTS_PATH.endswith('.pth') or WEIGHTS_PATH.endswith('.pt'):\n",
    "    transfer_weights = torch.load(WEIGHTS_PATH) \n",
    "else:\n",
    "    print('File type must be of (.pt, .pth, .onnx)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f2a0fd22-f59d-437c-8eb9-63ba3ce06526",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StyleTransferModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e5814271-6610-482b-a91b-5f1d2ce97d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_shapes = []\n",
    "for n, p in model.named_parameters():\n",
    "    weight_shapes.append((n, '-'.join([str(x) for x in list(p.shape)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e762758f-19e4-4e7e-8065-b73d3384e16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "down.0.weight\n",
      "torch.Size([128, 3, 7, 7]) \n",
      "\n",
      "down.0.bias\n",
      "torch.Size([128]) \n",
      "\n",
      "down.2.weight\n",
      "torch.Size([256, 128, 3, 3]) \n",
      "\n",
      "down.2.bias\n",
      "torch.Size([256]) \n",
      "\n",
      "down.4.weight\n",
      "torch.Size([512, 256, 3, 3]) \n",
      "\n",
      "down.4.bias\n",
      "torch.Size([512]) \n",
      "\n",
      "down.6.weight\n",
      "torch.Size([1024, 512, 3, 3]) \n",
      "\n",
      "down.6.bias\n",
      "torch.Size([1024]) \n",
      "\n",
      "style_blocks.0.conv1.conv.weight\n",
      "torch.Size([1024, 1024, 3, 3]) \n",
      "\n",
      "style_blocks.0.conv1.conv.bias\n",
      "torch.Size([1024]) \n",
      "\n",
      "style_blocks.0.conv1.adain.style.weight\n",
      "torch.Size([2048, 512]) \n",
      "\n",
      "style_blocks.0.conv1.adain.style.bias\n",
      "torch.Size([2048]) \n",
      "\n",
      "style_blocks.0.conv2.conv.weight\n",
      "torch.Size([1024, 1024, 3, 3]) \n",
      "\n",
      "style_blocks.0.conv2.conv.bias\n",
      "torch.Size([1024]) \n",
      "\n",
      "style_blocks.0.conv2.adain.style.weight\n",
      "torch.Size([2048, 512]) \n",
      "\n",
      "style_blocks.0.conv2.adain.style.bias\n",
      "torch.Size([2048]) \n",
      "\n",
      "style_blocks.1.conv1.conv.weight\n",
      "torch.Size([1024, 1024, 3, 3]) \n",
      "\n",
      "style_blocks.1.conv1.conv.bias\n",
      "torch.Size([1024]) \n",
      "\n",
      "style_blocks.1.conv1.adain.style.weight\n",
      "torch.Size([2048, 512]) \n",
      "\n",
      "style_blocks.1.conv1.adain.style.bias\n",
      "torch.Size([2048]) \n",
      "\n",
      "style_blocks.1.conv2.conv.weight\n",
      "torch.Size([1024, 1024, 3, 3]) \n",
      "\n",
      "style_blocks.1.conv2.conv.bias\n",
      "torch.Size([1024]) \n",
      "\n",
      "style_blocks.1.conv2.adain.style.weight\n",
      "torch.Size([2048, 512]) \n",
      "\n",
      "style_blocks.1.conv2.adain.style.bias\n",
      "torch.Size([2048]) \n",
      "\n",
      "style_blocks.2.conv1.conv.weight\n",
      "torch.Size([1024, 1024, 3, 3]) \n",
      "\n",
      "style_blocks.2.conv1.conv.bias\n",
      "torch.Size([1024]) \n",
      "\n",
      "style_blocks.2.conv1.adain.style.weight\n",
      "torch.Size([2048, 512]) \n",
      "\n",
      "style_blocks.2.conv1.adain.style.bias\n",
      "torch.Size([2048]) \n",
      "\n",
      "style_blocks.2.conv2.conv.weight\n",
      "torch.Size([1024, 1024, 3, 3]) \n",
      "\n",
      "style_blocks.2.conv2.conv.bias\n",
      "torch.Size([1024]) \n",
      "\n",
      "style_blocks.2.conv2.adain.style.weight\n",
      "torch.Size([2048, 512]) \n",
      "\n",
      "style_blocks.2.conv2.adain.style.bias\n",
      "torch.Size([2048]) \n",
      "\n",
      "style_blocks.3.conv1.conv.weight\n",
      "torch.Size([1024, 1024, 3, 3]) \n",
      "\n",
      "style_blocks.3.conv1.conv.bias\n",
      "torch.Size([1024]) \n",
      "\n",
      "style_blocks.3.conv1.adain.style.weight\n",
      "torch.Size([2048, 512]) \n",
      "\n",
      "style_blocks.3.conv1.adain.style.bias\n",
      "torch.Size([2048]) \n",
      "\n",
      "style_blocks.3.conv2.conv.weight\n",
      "torch.Size([1024, 1024, 3, 3]) \n",
      "\n",
      "style_blocks.3.conv2.conv.bias\n",
      "torch.Size([1024]) \n",
      "\n",
      "style_blocks.3.conv2.adain.style.weight\n",
      "torch.Size([2048, 512]) \n",
      "\n",
      "style_blocks.3.conv2.adain.style.bias\n",
      "torch.Size([2048]) \n",
      "\n",
      "style_blocks.4.conv1.conv.weight\n",
      "torch.Size([1024, 1024, 3, 3]) \n",
      "\n",
      "style_blocks.4.conv1.conv.bias\n",
      "torch.Size([1024]) \n",
      "\n",
      "style_blocks.4.conv1.adain.style.weight\n",
      "torch.Size([2048, 512]) \n",
      "\n",
      "style_blocks.4.conv1.adain.style.bias\n",
      "torch.Size([2048]) \n",
      "\n",
      "style_blocks.4.conv2.conv.weight\n",
      "torch.Size([1024, 1024, 3, 3]) \n",
      "\n",
      "style_blocks.4.conv2.conv.bias\n",
      "torch.Size([1024]) \n",
      "\n",
      "style_blocks.4.conv2.adain.style.weight\n",
      "torch.Size([2048, 512]) \n",
      "\n",
      "style_blocks.4.conv2.adain.style.bias\n",
      "torch.Size([2048]) \n",
      "\n",
      "style_blocks.5.conv1.conv.weight\n",
      "torch.Size([1024, 1024, 3, 3]) \n",
      "\n",
      "style_blocks.5.conv1.conv.bias\n",
      "torch.Size([1024]) \n",
      "\n",
      "style_blocks.5.conv1.adain.style.weight\n",
      "torch.Size([2048, 512]) \n",
      "\n",
      "style_blocks.5.conv1.adain.style.bias\n",
      "torch.Size([2048]) \n",
      "\n",
      "style_blocks.5.conv2.conv.weight\n",
      "torch.Size([1024, 1024, 3, 3]) \n",
      "\n",
      "style_blocks.5.conv2.conv.bias\n",
      "torch.Size([1024]) \n",
      "\n",
      "style_blocks.5.conv2.adain.style.weight\n",
      "torch.Size([2048, 512]) \n",
      "\n",
      "style_blocks.5.conv2.adain.style.bias\n",
      "torch.Size([2048]) \n",
      "\n",
      "up.1.weight\n",
      "torch.Size([512, 1024, 3, 3]) \n",
      "\n",
      "up.1.bias\n",
      "torch.Size([512]) \n",
      "\n",
      "up.4.weight\n",
      "torch.Size([256, 512, 3, 3]) \n",
      "\n",
      "up.4.bias\n",
      "torch.Size([256]) \n",
      "\n",
      "up.6.weight\n",
      "torch.Size([128, 256, 3, 3]) \n",
      "\n",
      "up.6.bias\n",
      "torch.Size([128]) \n",
      "\n",
      "up.8.weight\n",
      "torch.Size([3, 128, 7, 7]) \n",
      "\n",
      "up.8.bias\n",
      "torch.Size([3]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k, v in model.state_dict().items():\n",
    "    print(k)\n",
    "    print(v.shape, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5cd1ff18-e8ef-48af-88e4-3b370617403a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_encoder.0.weight\n",
      "torch.Size([128, 3, 7, 7]) \n",
      "\n",
      "target_encoder.0.bias\n",
      "torch.Size([128]) \n",
      "\n",
      "target_encoder.2.weight\n",
      "torch.Size([256, 128, 3, 3]) \n",
      "\n",
      "target_encoder.2.bias\n",
      "torch.Size([256]) \n",
      "\n",
      "target_encoder.4.weight\n",
      "torch.Size([512, 256, 3, 3]) \n",
      "\n",
      "target_encoder.4.bias\n",
      "torch.Size([512]) \n",
      "\n",
      "target_encoder.6.weight\n",
      "torch.Size([1024, 512, 3, 3]) \n",
      "\n",
      "target_encoder.6.bias\n",
      "torch.Size([1024]) \n",
      "\n",
      "style_blocks.0.conv1.weight\n",
      "torch.Size([1024, 1024, 3, 3]) \n",
      "\n",
      "style_blocks.0.conv1.bias\n",
      "torch.Size([1024]) \n",
      "\n",
      "style_blocks.0.conv2.weight\n",
      "torch.Size([1024, 1024, 3, 3]) \n",
      "\n",
      "style_blocks.0.conv2.bias\n",
      "torch.Size([1024]) \n",
      "\n",
      "style_blocks.0.style1.weight\n",
      "torch.Size([2048, 512]) \n",
      "\n",
      "style_blocks.0.style1.bias\n",
      "torch.Size([2048]) \n",
      "\n",
      "style_blocks.0.style2.weight\n",
      "torch.Size([2048, 512]) \n",
      "\n",
      "style_blocks.0.style2.bias\n",
      "torch.Size([2048]) \n",
      "\n",
      "style_blocks.1.conv1.weight\n",
      "torch.Size([1024, 1024, 3, 3]) \n",
      "\n",
      "style_blocks.1.conv1.bias\n",
      "torch.Size([1024]) \n",
      "\n",
      "style_blocks.1.conv2.weight\n",
      "torch.Size([1024, 1024, 3, 3]) \n",
      "\n",
      "style_blocks.1.conv2.bias\n",
      "torch.Size([1024]) \n",
      "\n",
      "style_blocks.1.style1.weight\n",
      "torch.Size([2048, 512]) \n",
      "\n",
      "style_blocks.1.style1.bias\n",
      "torch.Size([2048]) \n",
      "\n",
      "style_blocks.1.style2.weight\n",
      "torch.Size([2048, 512]) \n",
      "\n",
      "style_blocks.1.style2.bias\n",
      "torch.Size([2048]) \n",
      "\n",
      "style_blocks.2.conv1.weight\n",
      "torch.Size([1024, 1024, 3, 3]) \n",
      "\n",
      "style_blocks.2.conv1.bias\n",
      "torch.Size([1024]) \n",
      "\n",
      "style_blocks.2.conv2.weight\n",
      "torch.Size([1024, 1024, 3, 3]) \n",
      "\n",
      "style_blocks.2.conv2.bias\n",
      "torch.Size([1024]) \n",
      "\n",
      "style_blocks.2.style1.weight\n",
      "torch.Size([2048, 512]) \n",
      "\n",
      "style_blocks.2.style1.bias\n",
      "torch.Size([2048]) \n",
      "\n",
      "style_blocks.2.style2.weight\n",
      "torch.Size([2048, 512]) \n",
      "\n",
      "style_blocks.2.style2.bias\n",
      "torch.Size([2048]) \n",
      "\n",
      "style_blocks.3.conv1.weight\n",
      "torch.Size([1024, 1024, 3, 3]) \n",
      "\n",
      "style_blocks.3.conv1.bias\n",
      "torch.Size([1024]) \n",
      "\n",
      "style_blocks.3.conv2.weight\n",
      "torch.Size([1024, 1024, 3, 3]) \n",
      "\n",
      "style_blocks.3.conv2.bias\n",
      "torch.Size([1024]) \n",
      "\n",
      "style_blocks.3.style1.weight\n",
      "torch.Size([2048, 512]) \n",
      "\n",
      "style_blocks.3.style1.bias\n",
      "torch.Size([2048]) \n",
      "\n",
      "style_blocks.3.style2.weight\n",
      "torch.Size([2048, 512]) \n",
      "\n",
      "style_blocks.3.style2.bias\n",
      "torch.Size([2048]) \n",
      "\n",
      "style_blocks.4.conv1.weight\n",
      "torch.Size([1024, 1024, 3, 3]) \n",
      "\n",
      "style_blocks.4.conv1.bias\n",
      "torch.Size([1024]) \n",
      "\n",
      "style_blocks.4.conv2.weight\n",
      "torch.Size([1024, 1024, 3, 3]) \n",
      "\n",
      "style_blocks.4.conv2.bias\n",
      "torch.Size([1024]) \n",
      "\n",
      "style_blocks.4.style1.weight\n",
      "torch.Size([2048, 512]) \n",
      "\n",
      "style_blocks.4.style1.bias\n",
      "torch.Size([2048]) \n",
      "\n",
      "style_blocks.4.style2.weight\n",
      "torch.Size([2048, 512]) \n",
      "\n",
      "style_blocks.4.style2.bias\n",
      "torch.Size([2048]) \n",
      "\n",
      "style_blocks.5.conv1.weight\n",
      "torch.Size([1024, 1024, 3, 3]) \n",
      "\n",
      "style_blocks.5.conv1.bias\n",
      "torch.Size([1024]) \n",
      "\n",
      "style_blocks.5.conv2.weight\n",
      "torch.Size([1024, 1024, 3, 3]) \n",
      "\n",
      "style_blocks.5.conv2.bias\n",
      "torch.Size([1024]) \n",
      "\n",
      "style_blocks.5.style1.weight\n",
      "torch.Size([2048, 512]) \n",
      "\n",
      "style_blocks.5.style1.bias\n",
      "torch.Size([2048]) \n",
      "\n",
      "style_blocks.5.style2.weight\n",
      "torch.Size([2048, 512]) \n",
      "\n",
      "style_blocks.5.style2.bias\n",
      "torch.Size([2048]) \n",
      "\n",
      "decoder.0.weight\n",
      "torch.Size([512, 1024, 3, 3]) \n",
      "\n",
      "decoder.0.bias\n",
      "torch.Size([512]) \n",
      "\n",
      "decoderPart1.0.weight\n",
      "torch.Size([256, 512, 3, 3]) \n",
      "\n",
      "decoderPart1.0.bias\n",
      "torch.Size([256]) \n",
      "\n",
      "decoderPart1.2.weight\n",
      "torch.Size([128, 256, 3, 3]) \n",
      "\n",
      "decoderPart1.2.bias\n",
      "torch.Size([128]) \n",
      "\n",
      "decoderPart2.0.weight\n",
      "torch.Size([3, 128, 7, 7]) \n",
      "\n",
      "decoderPart2.0.bias\n",
      "torch.Size([3]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k, v in transfer_weights.items():\n",
    "    print(k)\n",
    "    print(v.shape, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5c1fc11c-9a86-4b61-a2cc-da4098c697bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if INSWAPPER:\n",
    "    replacement_dict = {\n",
    "        'styles': 'style_blocks',\n",
    "        'conv1.1': 'conv1.conv',\n",
    "        'style1.linear': 'conv1.adain.style',\n",
    "        'conv2.1': 'conv2.conv',\n",
    "        'style2.linear': 'conv2.adain.style',\n",
    "        'up0.1': 'up.8',\n",
    "        'onnx::Conv_833': 'down.0.weight',\n",
    "        'onnx::Conv_834': 'down.0.bias',\n",
    "        'onnx::Conv_836': 'down.2.weight',\n",
    "        'onnx::Conv_837': 'down.2.bias',\n",
    "        'onnx::Conv_839': 'down.4.weight',\n",
    "        'onnx::Conv_840': 'down.4.bias',\n",
    "        'onnx::Conv_842': 'down.6.weight',\n",
    "        'onnx::Conv_843': 'down.6.bias',\n",
    "        'onnx::Conv_845': 'up.1.weight',\n",
    "        'onnx::Conv_846': 'up.1.bias',\n",
    "        'onnx::Conv_848': 'up.4.weight',\n",
    "        'onnx::Conv_849': 'up.4.bias',\n",
    "        'onnx::Conv_851': 'up.6.weight',\n",
    "        'onnx::Conv_852': 'up.6.bias',\n",
    "        # 'initializer': 'initializer.weight'\n",
    "    }\n",
    "else:\n",
    "    replacement_dict = {\n",
    "        'target_encoder': 'down',\n",
    "        'conv1': 'conv1.conv',\n",
    "        'style1': 'conv1.adain.style',\n",
    "        'conv2': 'conv2.conv',\n",
    "        'style2': 'conv2.adain.style',\n",
    "        'decoder.0': 'up.1',\n",
    "        'decoderPart1.0': 'up.4',\n",
    "        'decoderPart1.2': 'up.6',\n",
    "        'decoderPart2.0': 'up.8'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "394cc119-aabb-469a-b6d5-68461318ff81",
   "metadata": {},
   "outputs": [],
   "source": [
    "renamed_weights = {}\n",
    "\n",
    "for k, v in transfer_weights.items():\n",
    "    orig_k = k\n",
    "    for name, replacement in replacement_dict.items():\n",
    "        k = k.replace(name, replacement)\n",
    "\n",
    "    if k == orig_k:\n",
    "        shape_name = '-'.join([str(x) for x in v.shape])\n",
    "        print(f'Shape of {k}: {v.shape}')\n",
    "        print(f'Possible names:')\n",
    "        replacements = []\n",
    "        for weight_shape in weight_shapes:\n",
    "            if shape_name == weight_shape[-1]:\n",
    "                print(f'  {weight_shape[0]}')\n",
    "                replacements.append(weight_shape[0])\n",
    "        if len(replacements) == 1:\n",
    "            k = replacements[0]\n",
    "\n",
    "    if k != orig_k:\n",
    "        renamed_weights[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a18d0860-ec77-4873-b66a-fca4ec846ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_33184\\3692548391.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state_dict = {k: torch.tensor(v) for k, v in renamed_weights.items()}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = {k: torch.tensor(v) for k, v in renamed_weights.items()}\n",
    "\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "833217af-1bc9-47b2-9753-e6f287777dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'inswapper_128_dict.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed102abf-68e3-45bf-87d8-d2f420712025",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'inswapper_128_cleaned.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6bd16763-bb1b-40c8-9bdb-646b99d4d978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 256, 256])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.rand((1, 3, 128, 128)), torch.rand((1, 512))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef33d15-24ea-49f4-a9ba-5c9be9bf814c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "tgt = torch.randn(1, 3, 128, 128)\n",
    "src_e = torch.randn(1, 512)\n",
    "\n",
    "y = model(tgt, src_e)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "torch.onnx.export(\n",
    "    model, \n",
    "    (tgt, src_e), \n",
    "    \"inswapper_eg.onnx\", \n",
    "    export_params=True,\n",
    "    opset_version=11, \n",
    "    input_names=['target', 'source'], \n",
    "    output_names=['output'], \n",
    "    dynamic_axes={'target': {0: 'batch_size'}, 'source': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3640dda9-754f-402d-8ba4-b6410a62660f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
